{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LlamaIndex provides comprehensive configuration support for the RAG process, allowing developers to personalize the entire process. Common configuration scenarios include:\n",
    "\n",
    "- Custom document chunk size(自定义文章分块大小)\n",
    "- Custom vector storage scheme (chroma, pycron)(自定义向量存储方案)\n",
    "- Custom query (the most common is to set the number of similar documents returned in matching similar documents)(自定义检索)\n",
    "- Specify LLM (generally defaults to openai, but not limited to openai)\n",
    "- Specify the response mode(only study how to set the mode today)\n",
    "- Specify streaming response (streaming response helps users have a better reading experience)(流式响应)\n",
    "\n",
    "Note that personalized configuration is mainly implemented through the **ServiceContext class** provided by LlamaIndex. (Not all are implemented by servicecontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "basic example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also expressed an interest in studying philosophy in college but eventually switched to AI.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "code samples for demonstrating LLaMAIndex's support for various configuration scenarious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to parse my documents into smaller chunks”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "service_context = ServiceContext.from_defaults(chunk_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合理调整chunk的大小有利于LLM模型更好的进行embedding，从而在用户进行query检索的时候输出与用户查询内容关联度更高的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to use a different vector store”\n",
    "_まだ理解なかった_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb #使用chroma作为存储向量的方案\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index import StorageContext\n",
    "\n",
    "chroma_client = chromadb.PersistentClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\") #chroma的客户端创建了一个集合叫做quickstart\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to retrieve more context when I query”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to use a different LLM”\n",
    "default to be openai, but can also use other LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import PaLM\n",
    "service_context = ServiceContext.from_defaults(llm=PaLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to use a different response mode”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer, a TRS-80, and started programming more extensively, writing simple games and a word processor. They initially planned to study philosophy in college but switched to AI. They also started publishing essays online and eventually wrote a book called \"Hackers & Painters.\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(response_mode='tree_summarize')\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree_summerize: summerize the main idea as a tree, summarize based on some documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tree_summerize:**\\\n",
    "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer, a TRS-80, and started programming more extensively, writing simple games and a word processor. They initially planned to study philosophy in college but switched to AI. They also started publishing essays online and eventually wrote a book called \"Hackers & Painters.\"\n",
    "\n",
    "compared with normal\n",
    "\n",
    "**normal**\\\n",
    "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also expressed an interest in studying philosophy in college but eventually switched to AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want to stream the response back”\n",
    "stream response is an user-friendly response way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI."
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### “I want a chatbot instead of Q&A”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to personal information about the author.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_chat_engine()\n",
    "response = query_engine.chat(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the query engine of chat, it will contine chatting with you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion. As an AI language model, I don't have real-time access to personal information about individuals unless it has been shared with me in the course of our conversation. I can provide general information and answer questions based on my training, but I don't have specific details about the author's personal life or experiences. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.chat(\"Oh interesting, tell me more.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Obtaining dependency information for chromadb from https://files.pythonhosted.org/packages/3c/ff/ac74735884031a3b9ddf7b1abecee0885ec61660588b1e7c6862bccf5116/chromadb-0.4.14-py3-none-any.whl.metadata\n",
      "  Downloading chromadb-0.4.14-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests>=2.28 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from chromadb) (2.4.2)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Obtaining dependency information for chroma-hnswlib==0.7.3 from https://files.pythonhosted.org/packages/cc/3d/ca311b8f79744db3f4faad8fd9140af80d34c94829d3ed1726c98cf4a611/chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Obtaining dependency information for fastapi>=0.95.2 from https://files.pythonhosted.org/packages/db/30/b8d323119c37e15b7fa639e65e0eb7d81eb675ba166ac83e695aad3bd321/fastapi-0.104.0-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.104.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Obtaining dependency information for uvicorn[standard]>=0.18.3 from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Obtaining dependency information for posthog>=2.4.0 from https://files.pythonhosted.org/packages/a7/73/35758818228c70348be4c3c66a76653c62e894e0e3c3461453c5341ca926/posthog-3.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading posthog-3.0.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from chromadb) (4.8.0)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Obtaining dependency information for pulsar-client>=3.1.0 from https://files.pythonhosted.org/packages/5b/41/56f3028c94b1082ff7142ec33e5f2760c8ecc611186e82b27b80cf7f1d73/pulsar_client-3.3.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading pulsar_client-3.3.0-cp310-cp310-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Obtaining dependency information for onnxruntime>=1.14.1 from https://files.pythonhosted.org/packages/97/26/c5b85935d80102e874fdd051a69bcedac15b6a9695efcafee4d2e1e690e3/onnxruntime-1.16.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading onnxruntime-1.16.1-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Obtaining dependency information for tokenizers>=0.13.2 from https://files.pythonhosted.org/packages/92/02/15556b80450301d2ef014bc598df4352bfb39631c5fcff758d8e0ac9f065/tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from chromadb) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Obtaining dependency information for overrides>=7.3.1 from https://files.pythonhosted.org/packages/da/28/3fa6ef8297302fc7b3844980b6c5dbc71cdbd4b61e9b2591234214d5ab39/overrides-7.4.0-py3-none-any.whl.metadata\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Obtaining dependency information for importlib-resources from https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Obtaining dependency information for grpcio>=1.58.0 from https://files.pythonhosted.org/packages/c7/40/d43d62a1da2d49700c3defddc307ca4b4e1a405a6ffd5fdacfcb4eea03da/grpcio-1.59.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-win_amd64.whl (152 kB)\n",
      "     ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 152.9/152.9 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.5 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: packaging in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/c2/59/f89c04923d68595d359f4cd7adbbdf5e5d791257945f8873d88b2fd1f979/protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     -------------- ------------------------- 2.1/5.7 MB 45.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 4.2/5.7 MB 45.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 45.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: certifi in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.28->chromadb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.28->chromadb) (1.26.17)\n",
      "Collecting huggingface_hub<0.18,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Obtaining dependency information for huggingface_hub<0.18,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for httptools>=0.5.0 from https://files.pythonhosted.org/packages/1e/fc/8a26c2adcd3f141e4729897633f03832b71ebea6f4c31cce67a92ded1961/httptools-0.6.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for watchfiles>=0.13 from https://files.pythonhosted.org/packages/4b/ea/80527adf1ad51488a96fc201715730af5879f4dfeccb5e2069ff82d890d4/watchfiles-0.21.0-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-win_amd64.whl (124 kB)\n",
      "     ---------------------------------------- 0.0/124.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 124.7/124.7 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
      "Collecting filelock (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\envs\\py310\\lib\\site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.9.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.8/86.8 kB ? eta 0:00:00\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 0.0/95.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 95.2/95.2 kB ? eta 0:00:00\n",
      "Downloading chromadb-0.4.14-py3-none-any.whl (448 kB)\n",
      "   ---------------------------------------- 0.0/448.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 448.1/448.1 kB ? eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 150.6/150.6 kB ? eta 0:00:00\n",
      "Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.9/92.9 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.59.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 3.4/3.7 MB 71.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 57.9 MB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.16.1-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 63.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.9/7.2 MB 62.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 50.8 MB/s eta 0:00:00\n",
      "Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
      "Downloading pulsar_client-3.3.0-cp310-cp310-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 3.2/3.4 MB 68.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 55.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.14.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.8/2.2 MB 58.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 34.7 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.2/58.2 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.0/295.0 kB ? eta 0:00:00\n",
      "Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/67.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 67.0/67.0 kB ? eta 0:00:00\n",
      "Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl (279 kB)\n",
      "   ---------------------------------------- 0.0/279.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 279.7/279.7 kB 16.8 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/430.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 430.5/430.5 kB 28.0 MB/s eta 0:00:00\n",
      "Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.5/59.5 kB ? eta 0:00:00\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=85c6f0fd66e628cb42304204e9758e7b37e275a6d35f55c5dfd3176e805a45cb\n",
      "  Stored in directory: c:\\users\\quan\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pyreadline3, pypika, mpmath, monotonic, flatbuffers, websockets, sympy, python-dotenv, pulsar-client, protobuf, overrides, importlib-resources, humanfriendly, httptools, h11, grpcio, filelock, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typer, starlette, posthog, huggingface_hub, coloredlogs, tokenizers, onnxruntime, fastapi, chromadb\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.14 coloredlogs-15.0.1 fastapi-0.104.0 filelock-3.12.4 flatbuffers-23.5.26 grpcio-1.59.0 h11-0.14.0 httptools-0.6.1 huggingface_hub-0.17.3 humanfriendly-10.0 importlib-resources-6.1.0 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.16.1 overrides-7.4.0 posthog-3.0.2 protobuf-4.24.4 pulsar-client-3.3.0 pypika-0.48.9 pyreadline3-3.4.1 python-dotenv-1.0.0 starlette-0.27.0 sympy-1.12 tokenizers-0.14.1 typer-0.9.0 uvicorn-0.23.2 watchfiles-0.21.0 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"]='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'\n",
    "\n",
    "import chromadb #使用chromadb存向量\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader #vectorstoreindex向量化， simpledirectoryreader从指定文件夹中加载文档\n",
    "from llama_index import ServiceContext #可以更改chunk大小和使用的llm等\n",
    "from llama_index.vector_stores import ChromaVectorStore \n",
    "from llama_index import StorageContext #向量存储配置\n",
    "from llama_index.llms import OpenAI #选择使用的llm模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一部分做自定义分块\n",
    "service_context = ServiceContext.from_defaults(chunk_size=500,llm=OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storagecontext自定义向量存储（StorageContext）\n",
    "chroma_client = chromadb.PersistentClient()\n",
    "chroma_collection_recent = chroma_client.create_collection(\"quickstart\")\n",
    "vector_store_recent = ChromaVectorStore(chroma_collection = chroma_collection_recent)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#索引文档\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n",
    "#将自定义的chunk数service_context和向量存储方式storage_content都导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author published essays online and wrote a book on Lisp. Additionally, the author worked as a painter and became a studio assistant for a painter named Idelle Weber."
     ]
    }
   ],
   "source": [
    "# 指定查询内容\n",
    "query_engine = index.as_query_engine(response_mode = \"tree_summarize\",streaming = True)\n",
    "response = query_engine.query(\"what did the auther do?\")\n",
    "response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
